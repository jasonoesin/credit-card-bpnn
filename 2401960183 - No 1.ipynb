{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jason\n",
    "<br>\n",
    "2401960183\n",
    "<br>\n",
    "Deep Learning - No 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Libraries Needed :\n",
    "- Numpy\n",
    "- Pandas\n",
    "- Tensorflow\n",
    "- Keras\n",
    "- Sklearn Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from sklearn.metrics import classification_report, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read data from creditcard.csv using pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>0.090794</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>-0.166974</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>0.207643</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>-0.054952</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>0.753074</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4       NaN  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9       V10  Class  \n",
       "0  0.098698  0.363787  0.090794      0  \n",
       "1  0.085102 -0.255425 -0.166974      0  \n",
       "2  0.247676 -1.514654  0.207643      0  \n",
       "3  0.377436 -1.387024 -0.054952      0  \n",
       "4 -0.270533  0.817739  0.753074      0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('creditcard.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the  total of rows and columns of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(284807, 11)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the info of each columns data type, here it shows that the Class has a int64 datatype because it is the value that determines if it is a fraud or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 284807 entries, 0 to 284806\n",
      "Data columns (total 11 columns):\n",
      " #   Column  Non-Null Count   Dtype  \n",
      "---  ------  --------------   -----  \n",
      " 0   V1      284802 non-null  float64\n",
      " 1   V2      284803 non-null  float64\n",
      " 2   V3      284801 non-null  float64\n",
      " 3   V4      284800 non-null  float64\n",
      " 4   V5      284801 non-null  float64\n",
      " 5   V6      284802 non-null  float64\n",
      " 6   V7      284802 non-null  float64\n",
      " 7   V8      284801 non-null  float64\n",
      " 8   V9      284799 non-null  float64\n",
      " 9   V10     284802 non-null  float64\n",
      " 10  Class   284807 non-null  int64  \n",
      "dtypes: float64(10), int64(1)\n",
      "memory usage: 23.9 MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>1a. Data Preprocesssing</h2>\n",
    "\n",
    "As i analyzes the dataset, there are some missing values across the dataframe. Therefore checking the total null count in each columns first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "V1       5\n",
       "V2       4\n",
       "V3       6\n",
       "V4       7\n",
       "V5       6\n",
       "V6       5\n",
       "V7       5\n",
       "V8       6\n",
       "V9       8\n",
       "V10      5\n",
       "Class    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop the rows that contains the null values and replace the original dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "V1       0\n",
       "V2       0\n",
       "V3       0\n",
       "V4       0\n",
       "V5       0\n",
       "V6       0\n",
       "V7       0\n",
       "V8       0\n",
       "V9       0\n",
       "V10      0\n",
       "Class    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.dropna()\n",
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the column V1 to V10 that it's input values have different scales which may affect the performance of the BPNN Model.\n",
    "The purpose of normalization is to convert the values of the dataset's numeric columns to a standard scale without losing information or distorting the ranges of values. Some algorithms need normalization in order to properly model the data. Therefore i am using the MinMaxScaler (Normalization) froim sklearn to normalize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "cols = data.columns\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "data = scaler.fit_transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After normalizing the data the column names are replaced, therefore replacing it with the original columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.935192</td>\n",
       "      <td>0.766490</td>\n",
       "      <td>0.881365</td>\n",
       "      <td>0.313023</td>\n",
       "      <td>0.763439</td>\n",
       "      <td>0.267669</td>\n",
       "      <td>0.266815</td>\n",
       "      <td>0.786444</td>\n",
       "      <td>0.475312</td>\n",
       "      <td>0.510600</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.978542</td>\n",
       "      <td>0.770067</td>\n",
       "      <td>0.840298</td>\n",
       "      <td>0.271796</td>\n",
       "      <td>0.766120</td>\n",
       "      <td>0.262192</td>\n",
       "      <td>0.264875</td>\n",
       "      <td>0.786298</td>\n",
       "      <td>0.453981</td>\n",
       "      <td>0.505267</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.935217</td>\n",
       "      <td>0.753118</td>\n",
       "      <td>0.868141</td>\n",
       "      <td>0.268766</td>\n",
       "      <td>0.762329</td>\n",
       "      <td>0.281122</td>\n",
       "      <td>0.270177</td>\n",
       "      <td>0.788042</td>\n",
       "      <td>0.410603</td>\n",
       "      <td>0.513018</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.941878</td>\n",
       "      <td>0.765304</td>\n",
       "      <td>0.868484</td>\n",
       "      <td>0.213661</td>\n",
       "      <td>0.765647</td>\n",
       "      <td>0.275559</td>\n",
       "      <td>0.266803</td>\n",
       "      <td>0.789434</td>\n",
       "      <td>0.414999</td>\n",
       "      <td>0.507585</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.979184</td>\n",
       "      <td>0.768746</td>\n",
       "      <td>0.838200</td>\n",
       "      <td>0.305241</td>\n",
       "      <td>0.767008</td>\n",
       "      <td>0.265762</td>\n",
       "      <td>0.265324</td>\n",
       "      <td>0.786257</td>\n",
       "      <td>0.478797</td>\n",
       "      <td>0.506668</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0  0.935192  0.766490  0.881365  0.313023  0.763439  0.267669  0.266815   \n",
       "1  0.978542  0.770067  0.840298  0.271796  0.766120  0.262192  0.264875   \n",
       "2  0.935217  0.753118  0.868141  0.268766  0.762329  0.281122  0.270177   \n",
       "3  0.941878  0.765304  0.868484  0.213661  0.765647  0.275559  0.266803   \n",
       "4  0.979184  0.768746  0.838200  0.305241  0.767008  0.265762  0.265324   \n",
       "\n",
       "         V8        V9       V10  Class  \n",
       "0  0.786444  0.475312  0.510600    0.0  \n",
       "1  0.786298  0.453981  0.505267    0.0  \n",
       "2  0.788042  0.410603  0.513018    0.0  \n",
       "3  0.789434  0.414999  0.507585    0.0  \n",
       "4  0.786257  0.478797  0.506668    0.0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.DataFrame(data, columns = cols.array)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>1b. Splitting Dataset</h2>\n",
    "\n",
    " Splitting the data into two parts first being the test set and residual set with 80% for test and 20% for residuals (consisting of test and validation). After that split the residual set into test and validation by giving the size 0.5 to split by two. Each having a total of 10% of the original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = data.drop(columns = ['Class']).copy()\n",
    "y = data['Class']\n",
    "\n",
    "\n",
    "X_train, X_res, y_train, y_res = train_test_split(X,y, train_size=0.8)\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X_res,y_res, test_size=0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>1c. Baseline Architecture of the BPNN Model</h2>\n",
    "\n",
    "First determining the total of nodes in the input layer. Here n is the total of input columns and n_class is the value to be predicted which consists of a binary values (0 and 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(X.columns) # 10\n",
    "n_class = len(y.unique()) # 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BPNN Model Building using Keras Sequential Model\n",
    "- First layer is the input layer that has n nodes (10) with activation function ReLU\n",
    "- Second layer is the hidden layer that has n*2 nodes (20) with activation function ReLU\n",
    "- Third layer is the hidden layer that has n*2 nodes (20) with activation function ReLU\n",
    "- Last layer is the output layer that has n_class nodes (2) with activation function of Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(n*2, input_shape=(n,), activation='relu'))\n",
    "model.add(Dense(n*2, activation='relu'))\n",
    "model.add(Dense(n_class, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the summary of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 20)                220       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 20)                420       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 2)                 42        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 682\n",
      "Trainable params: 682\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the Hyperparameters\n",
    "- Because the representation of \"Class\" values is 0 and 1 or (one hot encoding). Therefore in the event that there are two or more label classes, we will use this crossentropy loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.04\n",
    "epochs = 10\n",
    "loss = \"categorical_crossentropy\"\n",
    "metrics = ['accuracy']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile the model using ADAM Optimizer. Adaptive Moment Estimation is an algorithm for optimization technique. When dealing with complex problems involving a lot of data or factors, this optimizer is incredibly effective. It is effective and uses little memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(keras.optimizers.Adam(learning_rate=learning_rate), loss = loss ,metrics = metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we already have two classes (from y) that are integers, 1 meaning fraud, while 0 denotes not fraud. In order to reformat y into a 2-dimensional vector in terms of [1. 0.] or [0. 1.], one hot encoding will be used. This can be achieved by using keras.utils.to_categorical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_y_train = keras.utils.to_categorical(y_train,num_classes=None)\n",
    "vectorized_y_test = keras.utils.to_categorical(y_test,num_classes=None)\n",
    "vectorized_y_valid =  keras.utils.to_categorical(y_valid,num_classes=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the train dataset and validation data to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "7119/7119 [==============================] - 12s 2ms/step - loss: 0.0132 - accuracy: 0.9982 - val_loss: 0.0052 - val_accuracy: 0.9986\n",
      "Epoch 2/10\n",
      "7119/7119 [==============================] - 11s 2ms/step - loss: 0.0072 - accuracy: 0.9987 - val_loss: 0.0045 - val_accuracy: 0.9985\n",
      "Epoch 3/10\n",
      "7119/7119 [==============================] - 11s 2ms/step - loss: 0.0067 - accuracy: 0.9988 - val_loss: 0.0059 - val_accuracy: 0.9990\n",
      "Epoch 4/10\n",
      "7119/7119 [==============================] - 12s 2ms/step - loss: 0.0062 - accuracy: 0.9989 - val_loss: 0.0046 - val_accuracy: 0.9985\n",
      "Epoch 5/10\n",
      "7119/7119 [==============================] - 11s 2ms/step - loss: 0.0060 - accuracy: 0.9989 - val_loss: 0.0043 - val_accuracy: 0.9992\n",
      "Epoch 6/10\n",
      "7119/7119 [==============================] - 11s 2ms/step - loss: 0.0058 - accuracy: 0.9989 - val_loss: 0.0037 - val_accuracy: 0.9985\n",
      "Epoch 7/10\n",
      "7119/7119 [==============================] - 11s 2ms/step - loss: 0.0057 - accuracy: 0.9990 - val_loss: 0.0046 - val_accuracy: 0.9992\n",
      "Epoch 8/10\n",
      "7119/7119 [==============================] - 13s 2ms/step - loss: 0.0058 - accuracy: 0.9990 - val_loss: 0.0042 - val_accuracy: 0.9991\n",
      "Epoch 9/10\n",
      "7119/7119 [==============================] - 12s 2ms/step - loss: 0.0056 - accuracy: 0.9990 - val_loss: 0.0040 - val_accuracy: 0.9990\n",
      "Epoch 10/10\n",
      "7119/7119 [==============================] - 11s 2ms/step - loss: 0.0057 - accuracy: 0.9990 - val_loss: 0.0081 - val_accuracy: 0.9989\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c3a14e3970>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, vectorized_y_train, validation_data=(X_valid,vectorized_y_valid), epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>1e. Analyze the Performance of the Model</h2>\n",
    "\n",
    "Here to evaluate the model by using the sklearn metrics library. Accuracy can be found from the evaluate function provided by keras library to display the accuracy of the current model. Predicted values of the models are therefore evaluated to find the precision, recall, and f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "890/890 [==============================] - 1s 1ms/step - loss: 0.0083 - accuracy: 0.9989\n",
      "890/890 [==============================] - 1s 1ms/step\n",
      "Accuracy: 0.9989113807678223\n",
      "\n",
      "Fraud Precision:0.660377358490566\n",
      "Fraud Recall:0.7291666666666666\n",
      "Fraud F1-Score:0.693069306930693\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Normal  0.99954262 0.99936682 0.99945471     28428\n",
      "       Fraud  0.66037736 0.72916667 0.69306931        48\n",
      "\n",
      "    accuracy                      0.99891136     28476\n",
      "   macro avg  0.82995999 0.86426674 0.84626201     28476\n",
      "weighted avg  0.99897092 0.99891136 0.99893826     28476\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model():\n",
    "    labels=[\"Normal\",\"Fraud\"]\n",
    "\n",
    "    accuracy = model.evaluate(X_test,vectorized_y_test)[1]\n",
    "\n",
    "    prediction_control = model.predict(X_test, verbose=1)\n",
    "\n",
    "    y_pred = np.argmax(prediction_control,axis=1)\n",
    "    precision = precision_score(y_test,y_pred,labels=labels)\n",
    "    recall = recall_score(y_test,y_pred,labels=labels)\n",
    "    f1 = f1_score(y_test,y_pred,labels=labels)\n",
    "\n",
    "    print(\"Accuracy: {}\\n\".format(accuracy))\n",
    "    print(\"Fraud Precision:{}\".format(precision))\n",
    "    print(\"Fraud Recall:{}\".format(recall))\n",
    "    print(\"Fraud F1-Score:{}\\n\".format(f1))\n",
    "\n",
    "    print(classification_report(y_test,y_pred, target_names=labels,digits=8))\n",
    "\n",
    "\n",
    "evaluate_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>1d. Hyperparameters Tuning</h2>\n",
    "\n",
    "Based on the results of each epochs iteration, it shows that the loss values are mostly stagnan meaning that the learning rate is lowly affecting the results in each iteration, therefore here optimizing the learning rate is important.\n",
    "\n",
    "Tuned hyperparamaters :\n",
    "- Epochs. A gradient descent hyperparameter that regulates the quantity of full iterations across the training dataset is the number of epochs.\n",
    "- Batch Size. Gradient descent's batch size hyperparameter determines how many training data must be processed before the model's internal parameters are changed.\n",
    "- Learning Rate. When the model weights are changed, the learning rate is a hyperparameter that regulates how much to alter the model in response to the estimated error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "epochs = 10\n",
    "batch_size = 20\n",
    "loss = \"categorical_crossentropy\"\n",
    "metrics = ['accuracy']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile the model optimizers once again using AdamOptimizers with hyperparameters that were tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(keras.optimizers.Adam(learning_rate=learning_rate), loss = loss ,metrics = metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the train dataset and validation data to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "11391/11391 [==============================] - 20s 2ms/step - loss: 0.0041 - accuracy: 0.9991 - val_loss: 0.0035 - val_accuracy: 0.9992\n",
      "Epoch 2/10\n",
      "11391/11391 [==============================] - 19s 2ms/step - loss: 0.0040 - accuracy: 0.9991 - val_loss: 0.0035 - val_accuracy: 0.9991\n",
      "Epoch 3/10\n",
      "11391/11391 [==============================] - 19s 2ms/step - loss: 0.0040 - accuracy: 0.9991 - val_loss: 0.0036 - val_accuracy: 0.9992\n",
      "Epoch 4/10\n",
      "11391/11391 [==============================] - 19s 2ms/step - loss: 0.0040 - accuracy: 0.9991 - val_loss: 0.0036 - val_accuracy: 0.9992\n",
      "Epoch 5/10\n",
      "11391/11391 [==============================] - 18s 2ms/step - loss: 0.0040 - accuracy: 0.9991 - val_loss: 0.0035 - val_accuracy: 0.9991\n",
      "Epoch 6/10\n",
      "11391/11391 [==============================] - 19s 2ms/step - loss: 0.0040 - accuracy: 0.9991 - val_loss: 0.0038 - val_accuracy: 0.9991\n",
      "Epoch 7/10\n",
      "11391/11391 [==============================] - 18s 2ms/step - loss: 0.0040 - accuracy: 0.9991 - val_loss: 0.0036 - val_accuracy: 0.9991\n",
      "Epoch 8/10\n",
      "11391/11391 [==============================] - 19s 2ms/step - loss: 0.0040 - accuracy: 0.9991 - val_loss: 0.0035 - val_accuracy: 0.9992\n",
      "Epoch 9/10\n",
      "11391/11391 [==============================] - 19s 2ms/step - loss: 0.0040 - accuracy: 0.9991 - val_loss: 0.0035 - val_accuracy: 0.9991\n",
      "Epoch 10/10\n",
      "11391/11391 [==============================] - 20s 2ms/step - loss: 0.0040 - accuracy: 0.9991 - val_loss: 0.0035 - val_accuracy: 0.9991\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c3a7274b50>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, vectorized_y_train, validation_data=(X_valid,vectorized_y_valid), epochs=epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the hypertuned model using the defined function in earlier stages. Here as we can see from the results below we have a better performance after tuning the hyperparameters on the BPNN Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "890/890 [==============================] - 1s 1ms/step - loss: 0.0043 - accuracy: 0.9989\n",
      "890/890 [==============================] - 1s 1ms/step\n",
      "Accuracy: 0.998946487903595\n",
      "\n",
      "Fraud Precision:0.75\n",
      "Fraud Recall:0.5625\n",
      "Fraud F1-Score:0.6428571428571429\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Normal  0.99926160 0.99968341 0.99947246     28428\n",
      "       Fraud  0.75000000 0.56250000 0.64285714        48\n",
      "\n",
      "    accuracy                      0.99894648     28476\n",
      "   macro avg  0.87463080 0.78109171 0.82116480     28476\n",
      "weighted avg  0.99884144 0.99894648 0.99887134     28476\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "810eadbe619ca2ad193fd0c1c00936df6e00d92fae6c19534e04fffbf3fb2f81"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
